Model: lenet
Dataset: mnist
Learning Rate: 0.01
No.of Epochs: 50
Weight Decay: 0.0005
adv_train
==> Preparing data..
==> Building model..

Epoch: 0
Train:- Loss: 1.704 | Acc: 39.515% (23709/60000)
Test:-  Loss: 0.648 | Acc: 82.680% (8268/10000)
Saving..

Epoch: 1
Train:- Loss: 0.620 | Acc: 84.052% (50431/60000)
Test:-  Loss: 0.553 | Acc: 85.640% (8564/10000)
Saving..

Epoch: 2
Train:- Loss: 0.576 | Acc: 85.658% (51395/60000)
Test:-  Loss: 0.538 | Acc: 86.950% (8695/10000)
Saving..

Epoch: 3
Train:- Loss: 0.556 | Acc: 86.348% (51809/60000)
Test:-  Loss: 0.539 | Acc: 86.880% (8688/10000)

Epoch: 4
Train:- Loss: 0.546 | Acc: 86.680% (52008/60000)
Test:-  Loss: 0.526 | Acc: 87.160% (8716/10000)
Saving..

Epoch: 5
Train:- Loss: 0.541 | Acc: 86.865% (52119/60000)
Test:-  Loss: 0.531 | Acc: 86.930% (8693/10000)

Epoch: 6
Train:- Loss: 0.534 | Acc: 87.138% (52283/60000)
Test:-  Loss: 0.520 | Acc: 87.360% (8736/10000)
Saving..

Epoch: 7
Train:- Loss: 0.528 | Acc: 87.312% (52387/60000)
Test:-  Loss: 0.518 | Acc: 87.480% (8748/10000)
Saving..

Epoch: 8
Train:- Loss: 0.526 | Acc: 87.357% (52414/60000)
Test:-  Loss: 0.522 | Acc: 87.400% (8740/10000)

Epoch: 9
Train:- Loss: 0.524 | Acc: 87.403% (52442/60000)
Test:-  Loss: 0.519 | Acc: 87.530% (8753/10000)
Saving..

Epoch: 10
Train:- Loss: 0.523 | Acc: 87.448% (52469/60000)
Test:-  Loss: 0.523 | Acc: 87.420% (8742/10000)

Epoch: 11
Train:- Loss: 0.521 | Acc: 87.535% (52521/60000)
Test:-  Loss: 0.523 | Acc: 87.410% (8741/10000)

Epoch: 12
Train:- Loss: 0.518 | Acc: 87.628% (52577/60000)
Test:-  Loss: 0.517 | Acc: 87.570% (8757/10000)
Saving..

Epoch: 13
Train:- Loss: 0.517 | Acc: 87.652% (52591/60000)
Test:-  Loss: 0.515 | Acc: 87.760% (8776/10000)
Saving..

Epoch: 14
Train:- Loss: 0.516 | Acc: 87.720% (52632/60000)
Test:-  Loss: 0.514 | Acc: 87.470% (8747/10000)

Epoch: 15
Train:- Loss: 0.515 | Acc: 87.728% (52637/60000)
Test:-  Loss: 0.516 | Acc: 87.740% (8774/10000)

Epoch: 16
Train:- Loss: 0.514 | Acc: 87.843% (52706/60000)
Test:-  Loss: 0.513 | Acc: 87.660% (8766/10000)

Epoch: 17
Train:- Loss: 0.513 | Acc: 87.792% (52675/60000)
Test:-  Loss: 0.519 | Acc: 87.530% (8753/10000)

Epoch: 18
Train:- Loss: 0.512 | Acc: 87.848% (52709/60000)
Test:-  Loss: 0.514 | Acc: 87.690% (8769/10000)

Epoch: 19
Train:- Loss: 0.510 | Acc: 87.923% (52754/60000)
Test:-  Loss: 0.512 | Acc: 87.750% (8775/10000)

Epoch: 20
Train:- Loss: 0.510 | Acc: 87.873% (52724/60000)
Test:-  Loss: 0.517 | Acc: 87.770% (8777/10000)
Saving..

Epoch: 21
Train:- Loss: 0.510 | Acc: 87.898% (52739/60000)
Test:-  Loss: 0.514 | Acc: 87.650% (8765/10000)

Epoch: 22
Train:- Loss: 0.510 | Acc: 87.952% (52771/60000)
Test:-  Loss: 0.516 | Acc: 87.490% (8749/10000)

Epoch: 23
Train:- Loss: 0.510 | Acc: 87.883% (52730/60000)
Test:-  Loss: 0.516 | Acc: 87.620% (8762/10000)

Epoch: 24
Train:- Loss: 0.509 | Acc: 87.967% (52780/60000)
Test:-  Loss: 0.514 | Acc: 87.830% (8783/10000)
Saving..

Epoch: 25
Train:- Loss: 0.507 | Acc: 88.005% (52803/60000)
Test:-  Loss: 0.515 | Acc: 87.660% (8766/10000)

Epoch: 26
Train:- Loss: 0.507 | Acc: 88.020% (52812/60000)
Test:-  Loss: 0.514 | Acc: 87.770% (8777/10000)

Epoch: 27
Train:- Loss: 0.507 | Acc: 88.055% (52833/60000)
Test:-  Loss: 0.515 | Acc: 87.610% (8761/10000)

Epoch: 28
Train:- Loss: 0.506 | Acc: 88.082% (52849/60000)
Test:-  Loss: 0.513 | Acc: 87.830% (8783/10000)

Epoch: 29
Train:- Loss: 0.506 | Acc: 88.092% (52855/60000)
Test:-  Loss: 0.513 | Acc: 87.690% (8769/10000)

Epoch: 30
Train:- Loss: 0.506 | Acc: 88.065% (52839/60000)
Test:-  Loss: 0.519 | Acc: 87.540% (8754/10000)

Epoch: 31
Train:- Loss: 0.506 | Acc: 88.017% (52810/60000)
Test:-  Loss: 0.513 | Acc: 87.780% (8778/10000)

Epoch: 32
Train:- Loss: 0.505 | Acc: 88.093% (52856/60000)
Test:-  Loss: 0.511 | Acc: 87.870% (8787/10000)
Saving..

Epoch: 33
Train:- Loss: 0.505 | Acc: 88.117% (52870/60000)
Test:-  Loss: 0.511 | Acc: 87.740% (8774/10000)

Epoch: 34
Train:- Loss: 0.504 | Acc: 88.145% (52887/60000)
Test:-  Loss: 0.512 | Acc: 87.840% (8784/10000)

Epoch: 35
Train:- Loss: 0.504 | Acc: 88.122% (52873/60000)
Test:-  Loss: 0.512 | Acc: 87.700% (8770/10000)

Epoch: 36
Train:- Loss: 0.505 | Acc: 88.085% (52851/60000)
Test:-  Loss: 0.511 | Acc: 87.640% (8764/10000)

Epoch: 37
Train:- Loss: 0.504 | Acc: 88.132% (52879/60000)
Test:-  Loss: 0.512 | Acc: 87.680% (8768/10000)

Epoch: 38
Train:- Loss: 0.503 | Acc: 88.182% (52909/60000)
Test:-  Loss: 0.517 | Acc: 87.650% (8765/10000)

Epoch: 39
Train:- Loss: 0.503 | Acc: 88.143% (52886/60000)
Test:-  Loss: 0.515 | Acc: 87.610% (8761/10000)

Epoch: 40
Train:- Loss: 0.504 | Acc: 88.168% (52901/60000)
Test:-  Loss: 0.510 | Acc: 87.880% (8788/10000)
Saving..

Epoch: 41
Train:- Loss: 0.503 | Acc: 88.187% (52912/60000)
Test:-  Loss: 0.514 | Acc: 87.760% (8776/10000)

Epoch: 42
Train:- Loss: 0.503 | Acc: 88.145% (52887/60000)
Test:-  Loss: 0.511 | Acc: 87.640% (8764/10000)

Epoch: 43
Train:- Loss: 0.503 | Acc: 88.152% (52891/60000)
Test:-  Loss: 0.518 | Acc: 87.630% (8763/10000)

Epoch: 44
Train:- Loss: 0.502 | Acc: 88.190% (52914/60000)
Test:-  Loss: 0.510 | Acc: 87.780% (8778/10000)

Epoch: 45
Train:- Loss: 0.503 | Acc: 88.187% (52912/60000)
Test:-  Loss: 0.515 | Acc: 87.680% (8768/10000)

Epoch: 46
Train:- Loss: 0.503 | Acc: 88.148% (52889/60000)
Test:-  Loss: 0.518 | Acc: 87.560% (8756/10000)

Epoch: 47
Train:- Loss: 0.503 | Acc: 88.185% (52911/60000)
Test:-  Loss: 0.512 | Acc: 87.790% (8779/10000)

Epoch: 48
Train:- Loss: 0.501 | Acc: 88.225% (52935/60000)
Test:-  Loss: 0.513 | Acc: 87.710% (8771/10000)

Epoch: 49
Train:- Loss: 0.502 | Acc: 88.163% (52898/60000)
Test:-  Loss: 0.516 | Acc: 87.630% (8763/10000)
Completed
[1.7036277146608845, 0.6196169753445745, 0.5755166303056644, 0.5555281356008831, 0.5460933305815593, 0.540677707666146, 0.5338651154245904, 0.5283609498887937, 0.5264637519968852, 0.5236406074658132, 0.5228560776439811, 0.5211553102426691, 0.5182351626312809, 0.5172628887108902, 0.5159325992851369, 0.5146849637728002, 0.513543454473461, 0.5129969622820679, 0.5121024067340884, 0.5100357744104064, 0.510363257301451, 0.5104693135123517, 0.5096915639730405, 0.5097764731088935, 0.5085200332978895, 0.5074037276605553, 0.5069866744694171, 0.5065352087860295, 0.5055744501668761, 0.505589967073281, 0.5061602269662723, 0.5060354821495155, 0.5048491989991177, 0.5046530571192313, 0.5039789923853966, 0.5044044148979157, 0.505124315031683, 0.5035284405578174, 0.5026461254559091, 0.503386084872014, 0.5035338813562129, 0.5025414497708716, 0.503050768855157, 0.5028454627373071, 0.5016817091656393, 0.5032970399967135, 0.5031208218669078, 0.5026222223729722, 0.5014325415433597, 0.5022399819640717] [0.648421402000318, 0.5530830380643249, 0.537865734214236, 0.539230016386433, 0.5257285840951713, 0.5314345001035435, 0.5204743406954845, 0.5175899832871309, 0.5217466956111276, 0.5185189123745937, 0.523011591973578, 0.5227411024889369, 0.5167921179798758, 0.5149907665267871, 0.5139280462720591, 0.5155495663357389, 0.5125460757571421, 0.5194314963119046, 0.5137480968123029, 0.5121174384454253, 0.5172537650651993, 0.5142437798582065, 0.5158691117717962, 0.5163264775731761, 0.5135493849872783, 0.5154411405514759, 0.5140564538491, 0.5146053482772438, 0.5134435225823882, 0.5134787517748062, 0.5194865273442238, 0.5134942918826061, 0.510600040672691, 0.510792082472212, 0.5120033652159819, 0.5120463678791265, 0.5110722497390334, 0.512433013718599, 0.5173746571419345, 0.5148377820944331, 0.5096626668978649, 0.5143976792408402, 0.5110072856116447, 0.5177618971296177, 0.5104306818573339, 0.5150569990562026, 0.5182126545981997, 0.5118289682895515, 0.5128812469114923, 0.5157561324963904] [39.515, 84.05166666666666, 85.65833333333333, 86.34833333333333, 86.68, 86.865, 87.13833333333334, 87.31166666666667, 87.35666666666667, 87.40333333333334, 87.44833333333334, 87.535, 87.62833333333333, 87.65166666666667, 87.72, 87.72833333333334, 87.84333333333333, 87.79166666666667, 87.84833333333333, 87.92333333333333, 87.87333333333333, 87.89833333333333, 87.95166666666667, 87.88333333333334, 87.96666666666667, 88.005, 88.02, 88.055, 88.08166666666666, 88.09166666666667, 88.065, 88.01666666666667, 88.09333333333333, 88.11666666666666, 88.145, 88.12166666666667, 88.085, 88.13166666666666, 88.18166666666667, 88.14333333333333, 88.16833333333334, 88.18666666666667, 88.145, 88.15166666666667, 88.19, 88.18666666666667, 88.14833333333333, 88.185, 88.225, 88.16333333333333] [82.68, 85.64, 86.95, 86.88, 87.16, 86.93, 87.36, 87.48, 87.4, 87.53, 87.42, 87.41, 87.57, 87.76, 87.47, 87.74, 87.66, 87.53, 87.69, 87.75, 87.77, 87.65, 87.49, 87.62, 87.83, 87.66, 87.77, 87.61, 87.83, 87.69, 87.54, 87.78, 87.87, 87.74, 87.84, 87.7, 87.64, 87.68, 87.65, 87.61, 87.88, 87.76, 87.64, 87.63, 87.78, 87.68, 87.56, 87.79, 87.71, 87.63]
