Model: lenet
Dataset: mnist
Learning Rate: 0.01
No.of Epochs: 50
DropOut
==> Preparing data..
==> Building model..

Epoch: 0
Train:- Loss: 0.963 | Acc: 33.737% (20242/60000)
Test:-  Loss: 1.255 | Acc: 65.670% (6567/10000)
Saving..

Epoch: 1
Train:- Loss: 0.434 | Acc: 75.285% (45171/60000)
Test:-  Loss: 0.371 | Acc: 88.980% (8898/10000)
Saving..

Epoch: 2
Train:- Loss: 0.165 | Acc: 89.552% (53731/60000)
Test:-  Loss: 0.253 | Acc: 91.920% (9192/10000)
Saving..

Epoch: 3
Train:- Loss: 0.124 | Acc: 92.132% (55279/60000)
Test:-  Loss: 0.236 | Acc: 92.500% (9250/10000)
Saving..

Epoch: 4
Train:- Loss: 0.107 | Acc: 93.208% (55925/60000)
Test:-  Loss: 0.173 | Acc: 94.240% (9424/10000)
Saving..

Epoch: 5
Train:- Loss: 0.097 | Acc: 93.860% (56316/60000)
Test:-  Loss: 0.163 | Acc: 94.560% (9456/10000)
Saving..

Epoch: 6
Train:- Loss: 0.087 | Acc: 94.477% (56686/60000)
Test:-  Loss: 0.162 | Acc: 94.890% (9489/10000)
Saving..

Epoch: 7
Train:- Loss: 0.080 | Acc: 94.878% (56927/60000)
Test:-  Loss: 0.150 | Acc: 95.020% (9502/10000)
Saving..

Epoch: 8
Train:- Loss: 0.076 | Acc: 95.307% (57184/60000)
Test:-  Loss: 0.145 | Acc: 95.230% (9523/10000)
Saving..

Epoch: 9
Train:- Loss: 0.073 | Acc: 95.387% (57232/60000)
Test:-  Loss: 0.131 | Acc: 95.490% (9549/10000)
Saving..

Epoch: 10
Train:- Loss: 0.069 | Acc: 95.618% (57371/60000)
Test:-  Loss: 0.137 | Acc: 95.450% (9545/10000)

Epoch: 11
Train:- Loss: 0.067 | Acc: 95.800% (57480/60000)
Test:-  Loss: 0.123 | Acc: 96.100% (9610/10000)
Saving..

Epoch: 12
Train:- Loss: 0.066 | Acc: 95.823% (57494/60000)
Test:-  Loss: 0.122 | Acc: 95.910% (9591/10000)

Epoch: 13
Train:- Loss: 0.064 | Acc: 95.983% (57590/60000)
Test:-  Loss: 0.118 | Acc: 96.260% (9626/10000)
Saving..

Epoch: 14
Train:- Loss: 0.062 | Acc: 96.122% (57673/60000)
Test:-  Loss: 0.115 | Acc: 96.230% (9623/10000)

Epoch: 15
Train:- Loss: 0.060 | Acc: 96.207% (57724/60000)
Test:-  Loss: 0.122 | Acc: 96.080% (9608/10000)

Epoch: 16
Train:- Loss: 0.059 | Acc: 96.262% (57757/60000)
Test:-  Loss: 0.115 | Acc: 96.160% (9616/10000)

Epoch: 17
Train:- Loss: 0.058 | Acc: 96.302% (57781/60000)
Test:-  Loss: 0.118 | Acc: 96.170% (9617/10000)

Epoch: 18
Train:- Loss: 0.056 | Acc: 96.420% (57852/60000)
Test:-  Loss: 0.107 | Acc: 96.710% (9671/10000)
Saving..

Epoch: 19
Train:- Loss: 0.055 | Acc: 96.490% (57894/60000)
Test:-  Loss: 0.104 | Acc: 96.880% (9688/10000)
Saving..

Epoch: 20
Train:- Loss: 0.054 | Acc: 96.597% (57958/60000)
Test:-  Loss: 0.099 | Acc: 96.700% (9670/10000)

Epoch: 21
Train:- Loss: 0.055 | Acc: 96.533% (57920/60000)
Test:-  Loss: 0.102 | Acc: 96.630% (9663/10000)

Epoch: 22
Train:- Loss: 0.052 | Acc: 96.723% (58034/60000)
Test:-  Loss: 0.098 | Acc: 96.650% (9665/10000)

Epoch: 23
Train:- Loss: 0.052 | Acc: 96.683% (58010/60000)
Test:-  Loss: 0.097 | Acc: 96.770% (9677/10000)

Epoch: 24
Train:- Loss: 0.052 | Acc: 96.735% (58041/60000)
Test:-  Loss: 0.095 | Acc: 96.960% (9696/10000)
Saving..

Epoch: 25
Train:- Loss: 0.049 | Acc: 96.883% (58130/60000)
Test:-  Loss: 0.097 | Acc: 96.700% (9670/10000)

Epoch: 26
Train:- Loss: 0.049 | Acc: 96.902% (58141/60000)
Test:-  Loss: 0.103 | Acc: 96.650% (9665/10000)

Epoch: 27
Train:- Loss: 0.049 | Acc: 96.925% (58155/60000)
Test:-  Loss: 0.086 | Acc: 97.230% (9723/10000)
Saving..

Epoch: 28
Train:- Loss: 0.048 | Acc: 96.998% (58199/60000)
Test:-  Loss: 0.097 | Acc: 96.950% (9695/10000)

Epoch: 29
Train:- Loss: 0.048 | Acc: 96.953% (58172/60000)
Test:-  Loss: 0.090 | Acc: 97.070% (9707/10000)

Epoch: 30
Train:- Loss: 0.047 | Acc: 97.108% (58265/60000)
Test:-  Loss: 0.097 | Acc: 96.900% (9690/10000)

Epoch: 31
Train:- Loss: 0.046 | Acc: 97.083% (58250/60000)
Test:-  Loss: 0.106 | Acc: 96.540% (9654/10000)

Epoch: 32
Train:- Loss: 0.046 | Acc: 97.040% (58224/60000)
Test:-  Loss: 0.101 | Acc: 96.760% (9676/10000)

Epoch: 33
Train:- Loss: 0.045 | Acc: 97.173% (58304/60000)
Test:-  Loss: 0.092 | Acc: 96.980% (9698/10000)

Epoch: 34
Train:- Loss: 0.046 | Acc: 97.083% (58250/60000)
Test:-  Loss: 0.088 | Acc: 97.130% (9713/10000)

Epoch: 35
Train:- Loss: 0.045 | Acc: 97.183% (58310/60000)
Test:-  Loss: 0.089 | Acc: 97.050% (9705/10000)

Epoch: 36
Train:- Loss: 0.045 | Acc: 97.228% (58337/60000)
Test:-  Loss: 0.089 | Acc: 97.030% (9703/10000)

Epoch: 37
Train:- Loss: 0.044 | Acc: 97.190% (58314/60000)
Test:-  Loss: 0.090 | Acc: 97.130% (9713/10000)

Epoch: 38
Train:- Loss: 0.044 | Acc: 97.255% (58353/60000)
Test:-  Loss: 0.090 | Acc: 97.010% (9701/10000)

Epoch: 39
Train:- Loss: 0.043 | Acc: 97.298% (58379/60000)
Test:-  Loss: 0.095 | Acc: 96.950% (9695/10000)

Epoch: 40
Train:- Loss: 0.043 | Acc: 97.257% (58354/60000)
Test:-  Loss: 0.089 | Acc: 97.050% (9705/10000)

Epoch: 41
Train:- Loss: 0.043 | Acc: 97.280% (58368/60000)
Test:-  Loss: 0.090 | Acc: 97.160% (9716/10000)

Epoch: 42
Train:- Loss: 0.043 | Acc: 97.218% (58331/60000)
Test:-  Loss: 0.085 | Acc: 97.300% (9730/10000)
Saving..

Epoch: 43
Train:- Loss: 0.042 | Acc: 97.282% (58369/60000)
Test:-  Loss: 0.092 | Acc: 96.940% (9694/10000)

Epoch: 44
Train:- Loss: 0.041 | Acc: 97.410% (58446/60000)
Test:-  Loss: 0.089 | Acc: 96.990% (9699/10000)

Epoch: 45
Train:- Loss: 0.041 | Acc: 97.370% (58422/60000)
Test:-  Loss: 0.078 | Acc: 97.430% (9743/10000)
Saving..

Epoch: 46
Train:- Loss: 0.040 | Acc: 97.365% (58419/60000)
Test:-  Loss: 0.087 | Acc: 97.270% (9727/10000)

Epoch: 47
Train:- Loss: 0.041 | Acc: 97.425% (58455/60000)
Test:-  Loss: 0.083 | Acc: 97.370% (9737/10000)

Epoch: 48
Train:- Loss: 0.040 | Acc: 97.445% (58467/60000)
Test:-  Loss: 0.079 | Acc: 97.530% (9753/10000)
Saving..

Epoch: 49
Train:- Loss: 0.040 | Acc: 97.467% (58480/60000)
Test:-  Loss: 0.079 | Acc: 97.410% (9741/10000)
Completed
[0.9628151925896277, 0.4339019939271626, 0.16539274279210867, 0.12410259293293012, 0.10707149040529818, 0.09668519268078464, 0.0870974692933039, 0.0802515674350676, 0.07550811548449242, 0.07340085080294594, 0.06926655772426077, 0.06711629418376237, 0.06592871382115667, 0.06367372431611615, 0.061970293718223365, 0.06048415383713236, 0.05861762392946255, 0.05762463917375914, 0.05600542530739136, 0.05496722105334499, 0.05363884151750393, 0.055114868986132395, 0.05202909094070011, 0.051869149645989036, 0.05175769734911239, 0.04946820946983191, 0.048519193036442025, 0.04921041382079535, 0.047811907222385264, 0.04826252531460814, 0.046826088410327586, 0.046208937257490576, 0.04632703844494601, 0.04487958157383926, 0.04560189554219577, 0.04505874884070586, 0.04450680967122475, 0.04390689418494785, 0.04420202627563591, 0.04307105936564759, 0.043202027210346196, 0.04299398711415878, 0.04252408435076737, 0.042487848709314574, 0.04114714508770959, 0.04121814894853339, 0.04021700031222351, 0.04052152010455314, 0.039524037945719716, 0.0398395850581538] [1.2547064469118787, 0.3710456588750432, 0.2525696618138415, 0.2357595034691084, 0.1732667675152848, 0.16254969931854185, 0.16184891995849313, 0.14967871213236908, 0.14543956472793487, 0.13140193501095862, 0.13740087007498666, 0.12310382574470417, 0.12199211679102415, 0.11815773041186273, 0.1154547921281047, 0.1224514627844024, 0.11534497651878069, 0.11824273606124601, 0.10689329919345962, 0.1040823449266505, 0.09942935050500759, 0.10220549758345743, 0.09836835074416439, 0.09708071969580023, 0.09521921346361517, 0.09686579574255427, 0.102761730657754, 0.08560333406896728, 0.09662228141122373, 0.09027410716420502, 0.09661389509761695, 0.10611972404320599, 0.10106382317207813, 0.09228863991473676, 0.08814039505509494, 0.0892791317187837, 0.08883891833366257, 0.08952715345154143, 0.0900495274524074, 0.09518327831218737, 0.08882439146070463, 0.09039691457662814, 0.08474665420251501, 0.09212520557001921, 0.08945248598695561, 0.07782841897732727, 0.08673336016178089, 0.08272261290802105, 0.07850018031613391, 0.0788791014614165] [33.736666666666665, 75.285, 89.55166666666666, 92.13166666666666, 93.20833333333333, 93.86, 94.47666666666667, 94.87833333333333, 95.30666666666667, 95.38666666666667, 95.61833333333334, 95.8, 95.82333333333334, 95.98333333333333, 96.12166666666667, 96.20666666666666, 96.26166666666667, 96.30166666666666, 96.42, 96.49, 96.59666666666666, 96.53333333333333, 96.72333333333333, 96.68333333333334, 96.735, 96.88333333333334, 96.90166666666667, 96.925, 96.99833333333333, 96.95333333333333, 97.10833333333333, 97.08333333333333, 97.04, 97.17333333333333, 97.08333333333333, 97.18333333333334, 97.22833333333334, 97.19, 97.255, 97.29833333333333, 97.25666666666666, 97.28, 97.21833333333333, 97.28166666666667, 97.41, 97.37, 97.365, 97.425, 97.445, 97.46666666666667] [65.67, 88.98, 91.92, 92.5, 94.24, 94.56, 94.89, 95.02, 95.23, 95.49, 95.45, 96.1, 95.91, 96.26, 96.23, 96.08, 96.16, 96.17, 96.71, 96.88, 96.7, 96.63, 96.65, 96.77, 96.96, 96.7, 96.65, 97.23, 96.95, 97.07, 96.9, 96.54, 96.76, 96.98, 97.13, 97.05, 97.03, 97.13, 97.01, 96.95, 97.05, 97.16, 97.3, 96.94, 96.99, 97.43, 97.27, 97.37, 97.53, 97.41]
