/home/ridha/Documents/AI539Trustworthy/hw1copy
ridha
Sun Oct 29 09:48:33 PM PDT 2023
==============================================================================
Train LeNet on MNIST dataset
Model: lenet
Dataset: mnist
Learning Rate: 0.01
No.of Epochs: 50
Weight Decay: 0.0005
DropOut
==> Preparing data..
==> Building model..

Epoch: 0
Train:- Loss: 0.630 | Acc: 55.622% (33373/60000)
Test:-  Loss: 0.539 | Acc: 82.440% (8244/10000)
Saving..

Epoch: 1
Train:- Loss: 0.211 | Acc: 86.435% (51861/60000)
Test:-  Loss: 0.284 | Acc: 90.680% (9068/10000)
Saving..

Epoch: 2
Train:- Loss: 0.137 | Acc: 91.145% (54687/60000)
Test:-  Loss: 0.227 | Acc: 92.600% (9260/10000)
Saving..

Epoch: 3
Train:- Loss: 0.112 | Acc: 92.858% (55715/60000)
Test:-  Loss: 0.217 | Acc: 92.830% (9283/10000)
Saving..

Epoch: 4
Train:- Loss: 0.099 | Acc: 93.790% (56274/60000)
Test:-  Loss: 0.187 | Acc: 93.940% (9394/10000)
Saving..

Epoch: 5
Train:- Loss: 0.089 | Acc: 94.427% (56656/60000)
Test:-  Loss: 0.174 | Acc: 94.240% (9424/10000)
Saving..

Epoch: 6
Train:- Loss: 0.082 | Acc: 94.782% (56869/60000)
Test:-  Loss: 0.163 | Acc: 94.960% (9496/10000)
Saving..

Epoch: 7
Train:- Loss: 0.075 | Acc: 95.278% (57167/60000)
Test:-  Loss: 0.142 | Acc: 95.510% (9551/10000)
Saving..

Epoch: 8
Train:- Loss: 0.074 | Acc: 95.338% (57203/60000)
Test:-  Loss: 0.126 | Acc: 95.960% (9596/10000)
Saving..

Epoch: 9
Train:- Loss: 0.069 | Acc: 95.578% (57347/60000)
Test:-  Loss: 0.130 | Acc: 95.870% (9587/10000)

Epoch: 10
Train:- Loss: 0.067 | Acc: 95.738% (57443/60000)
Test:-  Loss: 0.119 | Acc: 96.200% (9620/10000)
Saving..

Epoch: 11
Train:- Loss: 0.065 | Acc: 95.932% (57559/60000)
Test:-  Loss: 0.128 | Acc: 95.870% (9587/10000)

Epoch: 12
Train:- Loss: 0.064 | Acc: 95.933% (57560/60000)
Test:-  Loss: 0.114 | Acc: 96.250% (9625/10000)
Saving..

Epoch: 13
Train:- Loss: 0.061 | Acc: 96.108% (57665/60000)
Test:-  Loss: 0.115 | Acc: 96.370% (9637/10000)
Saving..

Epoch: 14
Train:- Loss: 0.060 | Acc: 96.125% (57675/60000)
Test:-  Loss: 0.120 | Acc: 96.070% (9607/10000)

Epoch: 15
Train:- Loss: 0.058 | Acc: 96.365% (57819/60000)
Test:-  Loss: 0.106 | Acc: 96.590% (9659/10000)
Saving..

Epoch: 16
Train:- Loss: 0.056 | Acc: 96.490% (57894/60000)
Test:-  Loss: 0.105 | Acc: 96.510% (9651/10000)

Epoch: 17
Train:- Loss: 0.056 | Acc: 96.475% (57885/60000)
Test:-  Loss: 0.115 | Acc: 96.350% (9635/10000)

Epoch: 18
Train:- Loss: 0.054 | Acc: 96.583% (57950/60000)
Test:-  Loss: 0.119 | Acc: 96.140% (9614/10000)

Epoch: 19
Train:- Loss: 0.054 | Acc: 96.623% (57974/60000)
Test:-  Loss: 0.102 | Acc: 96.630% (9663/10000)
Saving..

Epoch: 20
Train:- Loss: 0.052 | Acc: 96.733% (58040/60000)
Test:-  Loss: 0.095 | Acc: 96.640% (9664/10000)
Saving..

Epoch: 21
Train:- Loss: 0.052 | Acc: 96.723% (58034/60000)
Test:-  Loss: 0.103 | Acc: 96.650% (9665/10000)
Saving..

Epoch: 22
Train:- Loss: 0.052 | Acc: 96.677% (58006/60000)
Test:-  Loss: 0.090 | Acc: 96.980% (9698/10000)
Saving..

Epoch: 23
Train:- Loss: 0.051 | Acc: 96.885% (58131/60000)
Test:-  Loss: 0.087 | Acc: 97.370% (9737/10000)
Saving..

Epoch: 24
Train:- Loss: 0.050 | Acc: 96.797% (58078/60000)
Test:-  Loss: 0.111 | Acc: 96.460% (9646/10000)

Epoch: 25
Train:- Loss: 0.049 | Acc: 96.977% (58186/60000)
Test:-  Loss: 0.101 | Acc: 96.730% (9673/10000)

Epoch: 26
Train:- Loss: 0.048 | Acc: 96.963% (58178/60000)
Test:-  Loss: 0.095 | Acc: 96.980% (9698/10000)

Epoch: 27
Train:- Loss: 0.048 | Acc: 96.987% (58192/60000)
Test:-  Loss: 0.102 | Acc: 96.660% (9666/10000)

Epoch: 28
Train:- Loss: 0.049 | Acc: 96.938% (58163/60000)
Test:-  Loss: 0.093 | Acc: 97.040% (9704/10000)

Epoch: 29
Train:- Loss: 0.047 | Acc: 97.088% (58253/60000)
Test:-  Loss: 0.092 | Acc: 97.130% (9713/10000)

Epoch: 30
Train:- Loss: 0.048 | Acc: 96.972% (58183/60000)
Test:-  Loss: 0.094 | Acc: 97.060% (9706/10000)

Epoch: 31
Train:- Loss: 0.046 | Acc: 97.157% (58294/60000)
Test:-  Loss: 0.095 | Acc: 96.900% (9690/10000)

Epoch: 32
Train:- Loss: 0.047 | Acc: 97.040% (58224/60000)
Test:-  Loss: 0.086 | Acc: 97.290% (9729/10000)

Epoch: 33
Train:- Loss: 0.046 | Acc: 97.107% (58264/60000)
Test:-  Loss: 0.094 | Acc: 96.950% (9695/10000)

Epoch: 34
Train:- Loss: 0.045 | Acc: 97.177% (58306/60000)
Test:-  Loss: 0.086 | Acc: 97.150% (9715/10000)

Epoch: 35
Train:- Loss: 0.045 | Acc: 97.147% (58288/60000)
Test:-  Loss: 0.091 | Acc: 97.070% (9707/10000)

Epoch: 36
Train:- Loss: 0.045 | Acc: 97.162% (58297/60000)
Test:-  Loss: 0.086 | Acc: 97.200% (9720/10000)

Epoch: 37
Train:- Loss: 0.044 | Acc: 97.245% (58347/60000)
Test:-  Loss: 0.096 | Acc: 96.870% (9687/10000)

Epoch: 38
Train:- Loss: 0.044 | Acc: 97.267% (58360/60000)
Test:-  Loss: 0.080 | Acc: 97.420% (9742/10000)
Saving..

Epoch: 39
Train:- Loss: 0.045 | Acc: 97.127% (58276/60000)
Test:-  Loss: 0.094 | Acc: 96.840% (9684/10000)

Epoch: 40
Train:- Loss: 0.042 | Acc: 97.315% (58389/60000)
Test:-  Loss: 0.089 | Acc: 97.290% (9729/10000)

Epoch: 41
Train:- Loss: 0.043 | Acc: 97.300% (58380/60000)
Test:-  Loss: 0.092 | Acc: 96.900% (9690/10000)

Epoch: 42
Train:- Loss: 0.043 | Acc: 97.335% (58401/60000)
Test:-  Loss: 0.092 | Acc: 96.980% (9698/10000)

Epoch: 43
Train:- Loss: 0.043 | Acc: 97.348% (58409/60000)
Test:-  Loss: 0.088 | Acc: 97.120% (9712/10000)

Epoch: 44
Train:- Loss: 0.041 | Acc: 97.275% (58365/60000)
Test:-  Loss: 0.083 | Acc: 97.480% (9748/10000)
Saving..

Epoch: 45
Train:- Loss: 0.043 | Acc: 97.282% (58369/60000)
Test:-  Loss: 0.079 | Acc: 97.630% (9763/10000)
Saving..

Epoch: 46
Train:- Loss: 0.041 | Acc: 97.395% (58437/60000)
Test:-  Loss: 0.077 | Acc: 97.720% (9772/10000)
Saving..

Epoch: 47
Train:- Loss: 0.042 | Acc: 97.303% (58382/60000)
Test:-  Loss: 0.088 | Acc: 97.240% (9724/10000)

Epoch: 48
Train:- Loss: 0.041 | Acc: 97.358% (58415/60000)
Test:-  Loss: 0.078 | Acc: 97.550% (9755/10000)

Epoch: 49
Train:- Loss: 0.039 | Acc: 97.513% (58508/60000)
Test:-  Loss: 0.079 | Acc: 97.390% (9739/10000)
Completed
[0.6304379139881907, 0.21066448008264305, 0.13691424686850898, 0.11201026022378634, 0.09864624783293462, 0.08864396185512895, 0.0815579795828665, 0.07483771290959738, 0.07355739982294313, 0.06949326437490899, 0.06685485758993433, 0.06487413715602937, 0.0637633274160405, 0.06123660188759052, 0.060097587909966485, 0.05770737343261253, 0.05625740707447844, 0.05571486426096386, 0.05439247106036135, 0.05395640641488612, 0.05246110723616043, 0.05173963908059423, 0.051560450193863405, 0.05079740873584623, 0.05031223136530534, 0.04899823216265683, 0.04813020170799324, 0.04766204068139354, 0.04867641679716231, 0.04729006502793224, 0.04768562701561534, 0.045762130470602895, 0.046526294193421205, 0.04586301345342417, 0.04460459317602932, 0.04494441315573432, 0.0453377534969966, 0.04375999287003651, 0.04373040751664282, 0.044689379633714096, 0.0424318121076484, 0.042872123111452436, 0.043039120256162085, 0.0425660092121006, 0.041449637128425434, 0.042977767568185866, 0.04093993966207544, 0.041568716890913356, 0.040920732200328415, 0.03917121121248822] [0.5391525411681765, 0.28419500418529386, 0.22724416338286962, 0.21658822770710964, 0.18737353421273126, 0.17384090136060384, 0.16271772213090377, 0.14249006305245837, 0.12589624878701272, 0.1297537213892848, 0.11867568256618206, 0.1277218884640154, 0.11378520706988823, 0.1148617296749548, 0.11997310795441601, 0.10573336563909509, 0.10485312369041097, 0.11543729550065769, 0.11892697852620986, 0.10197921677723906, 0.09498644805109947, 0.10322906676706543, 0.08984393429286944, 0.08735788385546915, 0.1112630473587745, 0.10074152592806869, 0.09511380325402222, 0.10167678324639157, 0.09276642002444122, 0.09192687346508051, 0.0935902943827989, 0.09548740653665203, 0.08593583071699283, 0.09437433094717562, 0.08646370576261572, 0.09130405616771525, 0.08606449228965019, 0.09625425204785933, 0.08046105447107374, 0.09442255491758608, 0.0888013857880987, 0.09173460487368261, 0.09235547162417418, 0.08769848492963442, 0.08285389284107976, 0.07910179428086664, 0.07691964444854137, 0.08783357560109632, 0.07821059408675506, 0.07924029885926195] [55.62166666666667, 86.435, 91.145, 92.85833333333333, 93.79, 94.42666666666666, 94.78166666666667, 95.27833333333334, 95.33833333333334, 95.57833333333333, 95.73833333333333, 95.93166666666667, 95.93333333333334, 96.10833333333333, 96.125, 96.365, 96.49, 96.475, 96.58333333333333, 96.62333333333333, 96.73333333333333, 96.72333333333333, 96.67666666666666, 96.885, 96.79666666666667, 96.97666666666667, 96.96333333333334, 96.98666666666666, 96.93833333333333, 97.08833333333334, 96.97166666666666, 97.15666666666667, 97.04, 97.10666666666667, 97.17666666666666, 97.14666666666666, 97.16166666666666, 97.245, 97.26666666666667, 97.12666666666667, 97.315, 97.3, 97.335, 97.34833333333333, 97.275, 97.28166666666667, 97.395, 97.30333333333333, 97.35833333333333, 97.51333333333334] [82.44, 90.68, 92.6, 92.83, 93.94, 94.24, 94.96, 95.51, 95.96, 95.87, 96.2, 95.87, 96.25, 96.37, 96.07, 96.59, 96.51, 96.35, 96.14, 96.63, 96.64, 96.65, 96.98, 97.37, 96.46, 96.73, 96.98, 96.66, 97.04, 97.13, 97.06, 96.9, 97.29, 96.95, 97.15, 97.07, 97.2, 96.87, 97.42, 96.84, 97.29, 96.9, 96.98, 97.12, 97.48, 97.63, 97.72, 97.24, 97.55, 97.39]
Sun Oct 29 09:52:41 PM PDT 2023
==============================================================================
==============================================================================
/home/ridha/Documents/AI539Trustworthy/hw1copy
ridha
Sun Nov  5 03:37:11 PM PST 2023
==============================================================================
Train LeNet on MNIST dataset
==============================================================================
Sun Nov  5 03:37:11 PM PST 2023
/home/ridha/Documents/AI539Trustworthy/hw1copy
ridha
Sun Nov  5 03:42:20 PM PST 2023
==============================================================================
Train LeNet on MNIST dataset
==============================================================================
Sun Nov  5 03:42:22 PM PST 2023
/home/ridha/Documents/AI539Trustworthy/hw1copy
ridha
Sun Nov  5 03:49:50 PM PST 2023
==============================================================================
Train LeNet on MNIST dataset
Model: lenet
Dataset: mnist
Learning Rate: 0.01
No.of Epochs: 50
Weight Decay: 0.0005
DropOut
==> Preparing data..
==> Building model..

Epoch: 0
Train:- Loss: 0.358 | Acc: 77.165% (46299/60000)
Test:-  Loss: 0.217 | Acc: 92.790% (9279/10000)
Saving..

Epoch: 1
Train:- Loss: 0.086 | Acc: 94.545% (56727/60000)
Test:-  Loss: 0.114 | Acc: 96.470% (9647/10000)
Saving..

Epoch: 2
Train:- Loss: 0.057 | Acc: 96.385% (57831/60000)
Test:-  Loss: 0.083 | Acc: 97.440% (9744/10000)
Saving..

Epoch: 3
Train:- Loss: 0.045 | Acc: 97.117% (58270/60000)
Test:-  Loss: 0.075 | Acc: 97.540% (9754/10000)
Saving..

Epoch: 4
Train:- Loss: 0.037 | Acc: 97.653% (58592/60000)
Test:-  Loss: 0.070 | Acc: 97.810% (9781/10000)
Saving..

Epoch: 5
Train:- Loss: 0.032 | Acc: 97.945% (58767/60000)
Test:-  Loss: 0.066 | Acc: 97.820% (9782/10000)
Saving..

Epoch: 6
Train:- Loss: 0.029 | Acc: 98.152% (58891/60000)
Test:-  Loss: 0.056 | Acc: 98.170% (9817/10000)
Saving..

Epoch: 7
Train:- Loss: 0.027 | Acc: 98.320% (58992/60000)
Test:-  Loss: 0.060 | Acc: 98.040% (9804/10000)

Epoch: 8
Train:- Loss: 0.024 | Acc: 98.400% (59040/60000)
/home/ridha/Documents/AI539Trustworthy/hw1copy
ridha
Sun Nov  5 03:55:09 PM PST 2023
==============================================================================
Train LeNet on MNIST dataset
==============================================================================
Sun Nov  5 03:55:09 PM PST 2023
/home/ridha/Documents/AI539Trustworthy/hw1copy
ridha
Sun Nov  5 03:55:36 PM PST 2023
==============================================================================
Train LeNet on MNIST dataset
/home/ridha/Documents/AI539Trustworthy/hw1copy
ridha
Sun Nov  5 03:57:09 PM PST 2023
==============================================================================
Train LeNet on MNIST dataset
==============================================================================
Sun Nov  5 03:57:12 PM PST 2023
/home/ridha/Documents/AI539Trustworthy/hw1copy
ridha
Sun Nov  5 04:16:50 PM PST 2023
==============================================================================
Train LeNet on MNIST dataset
==============================================================================
Sun Nov  5 04:20:55 PM PST 2023
