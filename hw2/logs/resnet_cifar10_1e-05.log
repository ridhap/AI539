========================================================================================================================
Model: resnet
Dataset: cifar10
Learning Rate: 0.01
No.of Epochs: 50
Weight Decay: 1e-05
DropOut
========================================================================================================================
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
========================================================================================================================
==> Building model..

Epoch: 0
Train:- Loss: 0.914 | Acc: 35.054% (17527/50000)
Test:-  Loss: 1.545 | Acc: 43.420% (4342/10000)
Saving..

Epoch: 1
Train:- Loss: 0.643 | Acc: 53.846% (26923/50000)
Test:-  Loss: 1.296 | Acc: 53.030% (5303/10000)
Saving..

Epoch: 2
Train:- Loss: 0.484 | Acc: 66.060% (33030/50000)
Test:-  Loss: 0.916 | Acc: 68.540% (6854/10000)
Saving..

Epoch: 3
Train:- Loss: 0.384 | Acc: 73.376% (36688/50000)
Test:-  Loss: 0.786 | Acc: 72.200% (7220/10000)
Saving..

Epoch: 4
Train:- Loss: 0.329 | Acc: 77.522% (38761/50000)
Test:-  Loss: 0.716 | Acc: 75.470% (7547/10000)
Saving..

Epoch: 5
Train:- Loss: 0.291 | Acc: 80.132% (40066/50000)
Test:-  Loss: 0.660 | Acc: 77.780% (7778/10000)
Saving..

Epoch: 6
Train:- Loss: 0.260 | Acc: 82.246% (41123/50000)
Test:-  Loss: 0.662 | Acc: 77.980% (7798/10000)
Saving..

Epoch: 7
Train:- Loss: 0.240 | Acc: 83.630% (41815/50000)
Test:-  Loss: 0.617 | Acc: 79.610% (7961/10000)
Saving..

Epoch: 8
Train:- Loss: 0.218 | Acc: 85.000% (42500/50000)
Test:-  Loss: 0.601 | Acc: 80.550% (8055/10000)
Saving..

Epoch: 9
Train:- Loss: 0.198 | Acc: 86.660% (43330/50000)
Test:-  Loss: 0.668 | Acc: 78.190% (7819/10000)

Epoch: 10
Train:- Loss: 0.184 | Acc: 87.450% (43725/50000)
Test:-  Loss: 0.593 | Acc: 81.120% (8112/10000)
Saving..

Epoch: 11
Train:- Loss: 0.172 | Acc: 88.206% (44103/50000)
Test:-  Loss: 0.611 | Acc: 80.840% (8084/10000)

Epoch: 12
Train:- Loss: 0.159 | Acc: 89.192% (44596/50000)
Test:-  Loss: 0.664 | Acc: 79.450% (7945/10000)

Epoch: 13
Train:- Loss: 0.150 | Acc: 89.706% (44853/50000)
Test:-  Loss: 0.754 | Acc: 79.360% (7936/10000)

Epoch: 14
Train:- Loss: 0.142 | Acc: 90.316% (45158/50000)
Test:-  Loss: 0.591 | Acc: 81.380% (8138/10000)
Saving..

Epoch: 15
Train:- Loss: 0.132 | Acc: 90.864% (45432/50000)
Test:-  Loss: 0.627 | Acc: 81.180% (8118/10000)

Epoch: 16
Train:- Loss: 0.126 | Acc: 91.312% (45656/50000)
Test:-  Loss: 0.667 | Acc: 81.200% (8120/10000)

Epoch: 17
Train:- Loss: 0.119 | Acc: 91.970% (45985/50000)
Test:-  Loss: 0.610 | Acc: 82.480% (8248/10000)
Saving..

Epoch: 18
Train:- Loss: 0.118 | Acc: 92.066% (46033/50000)
Test:-  Loss: 0.615 | Acc: 82.620% (8262/10000)
Saving..

Epoch: 19
Train:- Loss: 0.110 | Acc: 92.540% (46270/50000)
Test:-  Loss: 0.651 | Acc: 82.560% (8256/10000)

Epoch: 20
Train:- Loss: 0.108 | Acc: 92.534% (46267/50000)
Test:-  Loss: 0.654 | Acc: 82.080% (8208/10000)

Epoch: 21
Train:- Loss: 0.101 | Acc: 93.198% (46599/50000)
Test:-  Loss: 0.690 | Acc: 81.400% (8140/10000)

Epoch: 22
Train:- Loss: 0.100 | Acc: 93.310% (46655/50000)
Test:-  Loss: 0.681 | Acc: 82.130% (8213/10000)

Epoch: 23
Train:- Loss: 0.098 | Acc: 93.332% (46666/50000)
Test:-  Loss: 0.690 | Acc: 81.800% (8180/10000)

Epoch: 24
Train:- Loss: 0.095 | Acc: 93.682% (46841/50000)
Test:-  Loss: 0.701 | Acc: 82.440% (8244/10000)

Epoch: 25
Train:- Loss: 0.086 | Acc: 94.270% (47135/50000)
Test:-  Loss: 0.757 | Acc: 80.850% (8085/10000)

Epoch: 26
Train:- Loss: 0.096 | Acc: 93.514% (46757/50000)
Test:-  Loss: 0.726 | Acc: 81.290% (8129/10000)

Epoch: 27
Train:- Loss: 0.087 | Acc: 94.234% (47117/50000)
Test:-  Loss: 0.739 | Acc: 81.550% (8155/10000)

Epoch: 28
Train:- Loss: 0.085 | Acc: 94.396% (47198/50000)
Test:-  Loss: 0.755 | Acc: 80.750% (8075/10000)

Epoch: 29
Train:- Loss: 0.085 | Acc: 94.336% (47168/50000)
Test:-  Loss: 0.687 | Acc: 82.650% (8265/10000)
Saving..

Epoch: 30
Train:- Loss: 0.085 | Acc: 94.344% (47172/50000)
Test:-  Loss: 0.778 | Acc: 80.260% (8026/10000)

Epoch: 31
Train:- Loss: 0.084 | Acc: 94.330% (47165/50000)
Test:-  Loss: 0.887 | Acc: 78.820% (7882/10000)

Epoch: 32
Train:- Loss: 0.078 | Acc: 94.906% (47453/50000)
Test:-  Loss: 0.723 | Acc: 81.540% (8154/10000)

Epoch: 33
Train:- Loss: 0.078 | Acc: 94.816% (47408/50000)
Test:-  Loss: 0.740 | Acc: 81.480% (8148/10000)

Epoch: 34
Train:- Loss: 0.077 | Acc: 94.896% (47448/50000)
Test:-  Loss: 0.677 | Acc: 81.930% (8193/10000)

Epoch: 35
Train:- Loss: 0.077 | Acc: 95.112% (47556/50000)
Test:-  Loss: 0.706 | Acc: 81.950% (8195/10000)

Epoch: 36
Train:- Loss: 0.074 | Acc: 95.068% (47534/50000)
Test:-  Loss: 0.738 | Acc: 82.260% (8226/10000)

Epoch: 37
Train:- Loss: 0.077 | Acc: 94.834% (47417/50000)
Test:-  Loss: 0.699 | Acc: 81.680% (8168/10000)

Epoch: 38
Train:- Loss: 0.074 | Acc: 95.144% (47572/50000)
Test:-  Loss: 0.731 | Acc: 81.900% (8190/10000)

Epoch: 39
Train:- Loss: 0.075 | Acc: 94.982% (47491/50000)
Test:-  Loss: 0.716 | Acc: 81.750% (8175/10000)

Epoch: 40
Train:- Loss: 0.069 | Acc: 95.270% (47635/50000)
Test:-  Loss: 0.709 | Acc: 82.280% (8228/10000)

Epoch: 41
Train:- Loss: 0.068 | Acc: 95.496% (47748/50000)
Test:-  Loss: 0.791 | Acc: 81.120% (8112/10000)

Epoch: 42
Train:- Loss: 0.073 | Acc: 95.174% (47587/50000)
Test:-  Loss: 1.104 | Acc: 76.140% (7614/10000)

Epoch: 43
Train:- Loss: 0.069 | Acc: 95.400% (47700/50000)
Test:-  Loss: 0.747 | Acc: 81.660% (8166/10000)

Epoch: 44
Train:- Loss: 0.069 | Acc: 95.372% (47686/50000)
Test:-  Loss: 1.314 | Acc: 73.040% (7304/10000)

Epoch: 45
Train:- Loss: 0.063 | Acc: 95.776% (47888/50000)
Test:-  Loss: 0.764 | Acc: 82.160% (8216/10000)

Epoch: 46
Train:- Loss: 0.068 | Acc: 95.556% (47778/50000)
Test:-  Loss: 0.706 | Acc: 82.040% (8204/10000)

Epoch: 47
Train:- Loss: 0.063 | Acc: 95.820% (47910/50000)
Test:-  Loss: 0.818 | Acc: 80.940% (8094/10000)

Epoch: 48
Train:- Loss: 0.064 | Acc: 95.744% (47872/50000)
Test:-  Loss: 0.779 | Acc: 81.530% (8153/10000)

Epoch: 49
Train:- Loss: 0.064 | Acc: 95.820% (47910/50000)
Test:-  Loss: 0.794 | Acc: 81.070% (8107/10000)
========================================================================================================================
[0.9137741355487453, 0.6427896916866302, 0.483657055803577, 0.38410245853921643, 0.328615050143598, 0.29089091729629984, 0.260395252472147, 0.2397915263900824, 0.2175934463453567, 0.1976535187610199, 0.18434348820572924, 0.17192001938534057, 0.1593125787470728, 0.15015466543643372, 0.14171708566243843, 0.13218749068734592, 0.12597722085692997, 0.11873592100227656, 0.11757661019454298, 0.1099029078393641, 0.10833262727307656, 0.10132045725651105, 0.10008977059051014, 0.09822796290868993, 0.09452505094711394, 0.08625435628844878, 0.09599410516479055, 0.08653660806948724, 0.08503066603293227, 0.08467559489514441, 0.08454773178124024, 0.08410276479356925, 0.07774487849595287, 0.07842591558428258, 0.07743738904712089, 0.07723051236699456, 0.0744840080243633, 0.07738689374288216, 0.07362315170538357, 0.07491576215288008, 0.06938690689506719, 0.06820605954696017, 0.0731244364079288, 0.06890016570663475, 0.06902094264044319, 0.06339545237243442, 0.06828649781580033, 0.06284166503455871, 0.06399283461365611, 0.06387769651796926] [1.5445037867612899, 1.2961175373405407, 0.916022901702079, 0.7860120650689313, 0.7163843100617646, 0.6597361268511244, 0.6623009572363203, 0.616568102958096, 0.6006954840983555, 0.6680567670779624, 0.5928552780941034, 0.6106841727426857, 0.663552274180066, 0.753987401534038, 0.5907513409093686, 0.6267360512428223, 0.6674721741182789, 0.6103956089088112, 0.6145995197592268, 0.6512622661461496, 0.6535032597506881, 0.6899156103468245, 0.6807623844427667, 0.6901253477023666, 0.7007540966484956, 0.7567300315305685, 0.7260186130264003, 0.739052507717898, 0.755214352896259, 0.6866849813681499, 0.7782546888301327, 0.887177572128879, 0.7234395671232491, 0.7402377893590624, 0.6771435813539347, 0.70648480847383, 0.7382464234236699, 0.6985235647031456, 0.7313799875177396, 0.7159913690037029, 0.7093544951669729, 0.7913321491069855, 1.104346746282213, 0.7471712043710576, 1.3139456165064671, 0.7635745455504982, 0.7063959284572844, 0.8175736804297016, 0.7787689733657108, 0.7935765198651393] [35.054, 53.846, 66.06, 73.376, 77.522, 80.132, 82.246, 83.63, 85.0, 86.66, 87.45, 88.206, 89.192, 89.706, 90.316, 90.864, 91.312, 91.97, 92.066, 92.54, 92.534, 93.198, 93.31, 93.332, 93.682, 94.27, 93.514, 94.234, 94.396, 94.336, 94.344, 94.33, 94.906, 94.816, 94.896, 95.112, 95.068, 94.834, 95.144, 94.982, 95.27, 95.496, 95.174, 95.4, 95.372, 95.776, 95.556, 95.82, 95.744, 95.82] [43.42, 53.03, 68.54, 72.2, 75.47, 77.78, 77.98, 79.61, 80.55, 78.19, 81.12, 80.84, 79.45, 79.36, 81.38, 81.18, 81.2, 82.48, 82.62, 82.56, 82.08, 81.4, 82.13, 81.8, 82.44, 80.85, 81.29, 81.55, 80.75, 82.65, 80.26, 78.82, 81.54, 81.48, 81.93, 81.95, 82.26, 81.68, 81.9, 81.75, 82.28, 81.12, 76.14, 81.66, 73.04, 82.16, 82.04, 80.94, 81.53, 81.07]
